"""
Preprocessing Module
====================
Veri Ã¶n iÅŸleme ve eÄŸitim/test ayrÄ±mÄ±.

KRITIK NOKTALAR:
1. Look-ahead Bias (Gelecek Veri SÄ±zÄ±ntÄ±sÄ±) Ã–nleme
2. Log Returns kullanÄ±mÄ± (fiyat gÃ¼rÃ¼ltÃ¼sÃ¼nÃ¼ azaltÄ±r)
3. TimeSeriesSplit (zaman serisi iÃ§in doÄŸru CV)
4. MinMaxScaler (Ã¶zellikle LSTM iÃ§in gerekli)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import warnings
warnings.filterwarnings('ignore')


class DataPreprocessor:
    """
    Veri Ã¶n iÅŸleme ve train-test split iÅŸlemlerini yÃ¶neten sÄ±nÄ±f.
    """
    
    def __init__(self, df, target_col='Close'):
        """
        Args:
            df: Feature engineering sonrasÄ± veri
            target_col: Tahmin edilecek hedef kolon (default: 'Close')
        """
        self.df = df.copy()
        self.target_col = target_col
        self.scaler_X = None
        self.scaler_y = None
        
    def create_log_returns(self):
        """
        Logaritmik getiri (Log Returns) oluÅŸturur.
        
        NEDEN LOG RETURNS?
        1. Fiyat gÃ¼rÃ¼ltÃ¼sÃ¼nÃ¼ azaltÄ±r
        2. Stasyonerlik saÄŸlar (zaman serisi modelleme iÃ§in kritik)
        3. YÃ¼zde deÄŸiÅŸim mantÄ±ÄŸÄ±: log(P_t) - log(P_t-1) â‰ˆ (P_t - P_t-1) / P_t-1
        
        FormÃ¼l: log_return = ln(P_t / P_t-1) = ln(P_t) - ln(P_t-1)
        
        NOT: Model log return tahmin edecek, sonra geriye fiyata Ã§evireceÄŸiz.
        """
        print("ğŸ“‰ Log Returns hesaplanÄ±yor...")
        
        # Log return hesapla
        self.df['Log_Return'] = np.log(self.df[self.target_col] / 
                                       self.df[self.target_col].shift(1))
        
        # Ä°lk satÄ±rda NaN oluÅŸur (shift nedeniyle), drop edelim
        self.df = self.df.dropna(subset=['Log_Return'])
        
        print(f"âœ… Log Returns oluÅŸturuldu")
        print(f"   Mean: {self.df['Log_Return'].mean():.6f}")
        print(f"   Std: {self.df['Log_Return'].std():.6f}")
        
        return self.df
    
    def prepare_data(self, target='Log_Return', exclude_cols=None):
        """
        Veriyi X (features) ve y (target) olarak ayÄ±rÄ±r.
        
        Ã–NEMLI: Veri sÄ±zÄ±ntÄ±sÄ± (look-ahead bias) engelleme!
        - Target'Ä± hesaplarken gelecek bilgisi kullanÄ±lmaz
        - Sadece geÃ§miÅŸ verilerle tahmin yapÄ±lÄ±r
        
        Args:
            target: Hedef deÄŸiÅŸken (default: 'Log_Return')
            exclude_cols: X'ten hariÃ§ tutulacak kolonlar
            
        Returns:
            X, y: Features ve target
        """
        print("\nğŸ“Š X ve y ayrÄ±lÄ±yor...")
        
        # Hedef deÄŸiÅŸken var mÄ± kontrol et
        if target not in self.df.columns:
            raise ValueError(f"Hedef deÄŸiÅŸken '{target}' bulunamadÄ±!")
        
        # HariÃ§ tutulacak kolonlar
        if exclude_cols is None:
            exclude_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 
                          'Log_Return', 'Day_of_Week', 'Day_of_Month', 'Month', 'Quarter']
        
        # Feature seÃ§imi
        feature_cols = [col for col in self.df.columns if col not in exclude_cols]
        
        X = self.df[feature_cols].values
        y = self.df[target].values
        
        print(f"âœ… X shape: {X.shape}")
        print(f"âœ… y shape: {y.shape}")
        print(f"âœ… Feature sayÄ±sÄ±: {len(feature_cols)}")
        
        return X, y, feature_cols
    
    def timeseries_train_test_split(self, X, y, test_size=0.2):
        """
        Zaman serisi iÃ§in train-test split.
        
        UYARI: Zaman serisi verilerinde RANDOM SPLIT YAPILMAZ!
        - EÄŸitim verisi: GeÃ§miÅŸ (Ã¶rn: 2021-2023)
        - Test verisi: Gelecek (Ã¶rn: 2024)
        
        Neden?
        - Gelecek tahmini yapÄ±yoruz, geÃ§miÅŸe bakarak eÄŸitmeliyiz
        - Random split yaparsak "gelecekteki bilgi" ile eÄŸitmiÅŸ oluruz (veri sÄ±zÄ±ntÄ±sÄ±!)
        
        Args:
            X: Features
            y: Target
            test_size: Test set oranÄ± (default: 0.2)
            
        Returns:
            X_train, X_test, y_train, y_test
        """
        print("\nâœ‚ï¸ Train-Test Split (Zaman Serisi)...")
        
        # Split noktasÄ±
        split_idx = int(len(X) * (1 - test_size))
        
        X_train = X[:split_idx]
        X_test = X[split_idx:]
        y_train = y[:split_idx]
        y_test = y[split_idx:]
        
        print(f"âœ… Train: {X_train.shape[0]} Ã¶rneklem ({(1-test_size)*100:.0f}%)")
        print(f"âœ… Test: {X_test.shape[0]} Ã¶rneklem ({test_size*100:.0f}%)")
        print(f"\nâš ï¸ VERÄ° SIZINTISI ENGELLEME:")
        print(f"   - Train verisi: Ä°lk {split_idx} gÃ¼n (GEÃ‡MIÅ)")
        print(f"   - Test verisi: Son {len(X)-split_idx} gÃ¼n (GELECEK)")
        print(f"   - Gelecek verisi eÄŸitimde kullanÄ±lmadÄ±! âœ…")
        
        return X_train, X_test, y_train, y_test
    
    def get_timeseries_cv_splits(self, X, y, n_splits=5):
        """
        Zaman serisi iÃ§in cross-validation split'leri oluÅŸturur.
        
        TimeSeriesSplit:
        - Her split'te eÄŸitim seti bÃ¼yÃ¼r, test seti ileriye kayar
        - GerÃ§ek dÃ¼nya simÃ¼lasyonu: Her zaman geÃ§miÅŸten Ã¶ÄŸrenip geleceÄŸi tahmin ederiz
        
        Example (n_splits=3):
        Split 1: Train [0:100], Test [100:150]
        Split 2: Train [0:150], Test [150:200]
        Split 3: Train [0:200], Test [200:250]
        
        Args:
            X: Features
            y: Target
            n_splits: KaÃ§ CV fold (default: 5)
            
        Returns:
            TimeSeriesSplit object
        """
        print(f"\nğŸ“… TimeSeriesSplit ({n_splits} folds) hazÄ±rlanÄ±yor...")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        print(f"âœ… {n_splits} fold oluÅŸturuldu")
        print(f"   KullanÄ±m: model.cross_val_score(cv=tscv)")
        
        return tscv
    
    def scale_features(self, X_train, X_test, scaler_type='minmax'):
        """
        Ã–zellikleri Ã¶lÃ§eklendirir (Scaling).
        
        NEDEN SCALING?
        1. LightGBM: Zorunlu deÄŸil ama yardÄ±mcÄ± olur
        2. LSTM: ZORUNLU! (0-1 arasÄ± normalize edilmeli)
        3. FarklÄ± Ã¶lÃ§ekli deÄŸiÅŸkenler (Ã¶rn: Fiyat 50000, RSI 50) dengelenir
        
        Ã–NEMLÄ°: Scaler sadece TRAIN verisi ile fit edilir!
        - Test verisine aynÄ± transform uygulanÄ±r
        - BÃ¶ylece veri sÄ±zÄ±ntÄ±sÄ± engellenir
        
        Args:
            X_train: EÄŸitim features
            X_test: Test features
            scaler_type: 'minmax' veya 'standard'
            
        Returns:
            X_train_scaled, X_test_scaled
        """
        print(f"\nğŸ“ Feature Scaling ({scaler_type})...")
        
        if scaler_type == 'minmax':
            self.scaler_X = MinMaxScaler()
        elif scaler_type == 'standard':
            self.scaler_X = StandardScaler()
        else:
            raise ValueError("scaler_type 'minmax' veya 'standard' olmalÄ±")
        
        # FIT sadece train verisi ile!
        X_train_scaled = self.scaler_X.fit_transform(X_train)
        
        # Test verisine aynÄ± transform'u uygula
        X_test_scaled = self.scaler_X.transform(X_test)
        
        print(f"âœ… Scaling tamamlandÄ±")
        print(f"   Scaler: {scaler_type}")
        print(f"   Train Min-Max: [{X_train_scaled.min():.4f}, {X_train_scaled.max():.4f}]")
        print(f"   Test Min-Max: [{X_test_scaled.min():.4f}, {X_test_scaled.max():.4f}]")
        print(f"\nâš ï¸ VERÄ° SIZINTISI ENGELLEME:")
        print(f"   - Scaler SADECE train verisi ile fit edildi âœ…")
        print(f"   - Test verisi fit sÄ±rasÄ±nda kullanÄ±lmadÄ± âœ…")
        
        return X_train_scaled, X_test_scaled
    
    def scale_target(self, y_train, y_test):
        """
        Hedef deÄŸiÅŸkeni Ã¶lÃ§eklendirir (LSTM iÃ§in gerekli).
        
        Args:
            y_train: EÄŸitim target
            y_test: Test target
            
        Returns:
            y_train_scaled, y_test_scaled
        """
        print("\nğŸ¯ Target Scaling...")
        
        self.scaler_y = MinMaxScaler()
        
        # Target'Ä± 2D array'e Ã§evir (scaler iÃ§in gerekli)
        y_train = y_train.reshape(-1, 1)
        y_test = y_test.reshape(-1, 1)
        
        # Fit sadece train ile
        y_train_scaled = self.scaler_y.fit_transform(y_train)
        y_test_scaled = self.scaler_y.transform(y_test)
        
        print(f"âœ… Target scaling tamamlandÄ±")
        print(f"   Train Min-Max: [{y_train_scaled.min():.4f}, {y_train_scaled.max():.4f}]")
        
        return y_train_scaled.flatten(), y_test_scaled.flatten()
    
    def inverse_transform_target(self, y_scaled):
        """
        Ã–lÃ§eklendirilmiÅŸ target'Ä± orijinal Ã¶lÃ§eÄŸe geri dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            y_scaled: Scaled target deÄŸerleri
            
        Returns:
            Original scale target
        """
        if self.scaler_y is None:
            raise ValueError("Scaler henÃ¼z fit edilmemiÅŸ!")
        
        y_scaled = y_scaled.reshape(-1, 1)
        y_original = self.scaler_y.inverse_transform(y_scaled)
        return y_original.flatten()
    
    def inverse_transform_features(self, X_scaled):
        """
        Ã–lÃ§eklendirilmiÅŸ features'Ä± orijinal Ã¶lÃ§eÄŸe geri dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            X_scaled: Scaled features
            
        Returns:
            Original scale features
        """
        if self.scaler_X is None:
            raise ValueError("Scaler henÃ¼z fit edilmemiÅŸ!")
        
        return self.scaler_X.inverse_transform(X_scaled)
    
    def log_return_to_price(self, log_returns, initial_price):
        """
        Log return'leri fiyata Ã§evirir.
        
        FormÃ¼l: P_t = P_{t-1} * exp(log_return_t)
        
        Args:
            log_returns: Log return dizisi
            initial_price: BaÅŸlangÄ±Ã§ fiyatÄ±
            
        Returns:
            Fiyat dizisi
        """
        prices = [initial_price]
        
        for lr in log_returns:
            new_price = prices[-1] * np.exp(lr)
            prices.append(new_price)
        
        return np.array(prices[1:])  # Ä°lk deÄŸeri Ã§Ä±kar
    
    def prepare_lstm_sequences(self, X, y, seq_length=60):
        """
        LSTM iÃ§in sequence (dizi) oluÅŸturur.
        
        LSTM'in Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±:
        - Sequence: Son N gÃ¼nÃ¼n verisi â†’ YarÄ±nÄ±n tahmini
        - Ã–rnek: Son 60 gÃ¼nÃ¼ ver â†’ 61. gÃ¼nÃ¼ tahmin et
        
        Args:
            X: Features
            y: Target
            seq_length: Sequence uzunluÄŸu (default: 60)
            
        Returns:
            X_sequences, y_sequences
        """
        print(f"\nğŸ”— LSTM Sequences oluÅŸturuluyor (seq_length={seq_length})...")
        
        X_seq = []
        y_seq = []
        
        for i in range(seq_length, len(X)):
            X_seq.append(X[i-seq_length:i])  # Son N gÃ¼n
            y_seq.append(y[i])  # N+1. gÃ¼nÃ¼n deÄŸeri
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"âœ… Sequences oluÅŸturuldu")
        print(f"   X shape: {X_seq.shape} (samples, timesteps, features)")
        print(f"   y shape: {y_seq.shape}")
        
        return X_seq, y_seq


class FullPipeline:
    """
    TÃ¼m preprocessing adÄ±mlarÄ±nÄ± tek seferde Ã§alÄ±ÅŸtÄ±ran wrapper sÄ±nÄ±f.
    """
    
    def __init__(self, featured_df, target_col='Close'):
        """
        Args:
            featured_df: Feature engineering sonrasÄ± DataFrame
            target_col: Tahmin edilecek kolon
        """
        self.preprocessor = DataPreprocessor(featured_df, target_col)
    
    def run_lightgbm_pipeline(self, test_size=0.2, scaler_type='minmax'):
        """
        LightGBM iÃ§in tam preprocessing pipeline.
        
        Returns:
            Dict: TÃ¼m gerekli veriler
        """
        print("\n" + "="*60)
        print("ğŸ”§ LIGHTGBM PREPROCESSING PIPELINE")
        print("="*60)
        
        # 1. Log returns oluÅŸtur
        self.preprocessor.create_log_returns()
        
        # 2. X, y ayÄ±r
        X, y, feature_names = self.preprocessor.prepare_data(target='Log_Return')
        
        # 3. Train-test split (zaman serisi)
        X_train, X_test, y_train, y_test = \
            self.preprocessor.timeseries_train_test_split(X, y, test_size)
        
        # 4. Scaling (opsiyonel ama tavsiye edilir)
        X_train_scaled, X_test_scaled = \
            self.preprocessor.scale_features(X_train, X_test, scaler_type)
        
        print("\nâœ… LightGBM preprocessing tamamlandÄ±!")
        
        return {
            'X_train': X_train_scaled,
            'X_test': X_test_scaled,
            'y_train': y_train,
            'y_test': y_test,
            'feature_names': feature_names,
            'preprocessor': self.preprocessor,
            'original_df': self.preprocessor.df
        }
    
    def run_lstm_pipeline(self, test_size=0.2, seq_length=60):
        """
        LSTM iÃ§in tam preprocessing pipeline.
        
        Returns:
            Dict: TÃ¼m gerekli veriler
        """
        print("\n" + "="*60)
        print("ğŸ”§ LSTM PREPROCESSING PIPELINE")
        print("="*60)
        
        # 1. Log returns oluÅŸtur
        self.preprocessor.create_log_returns()
        
        # 2. X, y ayÄ±r
        X, y, feature_names = self.preprocessor.prepare_data(target='Log_Return')
        
        # 3. Train-test split
        X_train, X_test, y_train, y_test = \
            self.preprocessor.timeseries_train_test_split(X, y, test_size)
        
        # 4. Scaling (LSTM iÃ§in zorunlu!)
        X_train_scaled, X_test_scaled = \
            self.preprocessor.scale_features(X_train, X_test, 'minmax')
        
        # 5. Target scaling
        y_train_scaled, y_test_scaled = \
            self.preprocessor.scale_target(y_train, y_test)
        
        # 6. LSTM sequences
        X_train_seq, y_train_seq = \
            self.preprocessor.prepare_lstm_sequences(X_train_scaled, y_train_scaled, seq_length)
        X_test_seq, y_test_seq = \
            self.preprocessor.prepare_lstm_sequences(X_test_scaled, y_test_scaled, seq_length)
        
        print("\nâœ… LSTM preprocessing tamamlandÄ±!")
        
        return {
            'X_train': X_train_seq,
            'X_test': X_test_seq,
            'y_train': y_train_seq,
            'y_test': y_test_seq,
            'feature_names': feature_names,
            'preprocessor': self.preprocessor,
            'original_df': self.preprocessor.df,
            'seq_length': seq_length
        }


if __name__ == "__main__":
    # Test kodu
    from data_loader import DataLoader
    from feature_engineering import FeatureEngineer
    
    # Veri yÃ¼kle
    loader = DataLoader()
    raw_data = loader.merge_all_data()
    
    # Feature engineering
    engineer = FeatureEngineer(raw_data)
    featured_data = engineer.create_all_features(n_lags=30)
    
    # LightGBM pipeline test
    pipeline = FullPipeline(featured_data)
    lgb_data = pipeline.run_lightgbm_pipeline()
    
    print(f"\nğŸ“Š LightGBM HazÄ±r Veri:")
    print(f"   X_train: {lgb_data['X_train'].shape}")
    print(f"   y_train: {lgb_data['y_train'].shape}")
